{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import google.generativeai\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2f56b-127d-495c-b28d-3e8bb5d82e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "MODEL=\"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cf478-c118-4de8-b585-53b73823aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_ollama(prompt, model_name=MODEL):\n",
    "    try:\n",
    "        response = ollama.chat(model=MODEL, messages=[{'role': 'user', 'content': prompt}], stream=True)\n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            # Check if 'message' key exists and 'content' key exists within 'message'\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                result += chunk['message']['content']\n",
    "                yield result\n",
    "            # Optionally handle chunks that don't have the expected structure\n",
    "            # else:\n",
    "            #     print(f\"Skipping chunk with unexpected structure: {chunk}\")\n",
    "    except Exception as e:\n",
    "        yield f\"Error generating response from Ollama ({model_name}): {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key not set\n",
      "Anthropic API Key not set\n",
      "Google API Key exists and begins AIzaSyCn\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "# Student Octavio O. has pointed out that this isn't quite as straightforward for Claude -\n",
    "# see the excellent contribution in community-contributions \"Gradio_issue_with_Claude\" that handles Claude.\n",
    "import google.generativeai as genai\n",
    "# Define the system message outside the function if it's constant\n",
    "system_message = \"You are a helpful AI assistant.\"\n",
    "\n",
    "def chat(message, history):\n",
    "  # Construct the messages list, ensuring history elements are also correctly formatted\n",
    "  # The error indicates the structure of history items is not as expected by the API\n",
    "  # We need to convert history from Gradio's format (usually list of lists [user_msg, bot_msg])\n",
    "  # to the format expected by Gemini API: list of dicts [{\"role\": \"...\", \"parts\": [...]}]\n",
    "\n",
    "  formatted_history = []\n",
    "  for turn in history:\n",
    "    # Gradio history can sometimes have odd structures, ensure we handle single elements too\n",
    "    if isinstance(turn, list) and len(turn) == 2:\n",
    "        human, ai = turn\n",
    "        formatted_history.append({\"role\": \"user\", \"parts\": [human]})\n",
    "        formatted_history.append({\"role\": \"model\", \"parts\": [ai]}) # Gemini uses 'model' for AI\n",
    "    elif isinstance(turn, str): # Handle cases where a turn might just be a string (less common for history but good practice)\n",
    "         formatted_history.append({\"role\": \"user\", \"parts\": [turn]})\n",
    "    # Add other handling for different history structures if needed\n",
    "\n",
    "  messages = [{\"role\": \"user\", \"parts\": [system_message]}] + formatted_history + [{\"role\": \"user\", \"parts\": [message]}]\n",
    "\n",
    "  print(\"History is:\")\n",
    "  print(history)\n",
    "  print(\"And messages is:\")\n",
    "  print(messages)\n",
    "\n",
    "  model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "  stream = model.generate_content(messages, stream=True)\n",
    "  response = \"\"\n",
    "  for chunk in stream:\n",
    "    response += chunk.text\n",
    "    yield response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'user', 'parts': ['You are a helpful AI assistant.']}, {'role': 'user', 'parts': ['f']}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'f', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"I'm ready for your request.  What can I help you with?\", 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'user', 'parts': ['You are a helpful AI assistant.']}, {'role': 'user', 'parts': ['dd']}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'f', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"I'm ready for your request.  What can I help you with?\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'dd', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '\"dd\" is a command-line utility in Unix-like operating systems.  It\\'s used for copying and converting files.  Because it\\'s powerful and can easily overwrite data, it\\'s important to use it carefully.  Without more context, I can\\'t tell you what you want to do with `dd`.  To help me help you, please tell me what you\\'d like to use `dd` for.  For example:\\n\\n* **Are you trying to create a disk image?**  (e.g., `dd if=/dev/sda of=image.img`)\\n* **Are you trying to copy a file, potentially converting its format?** (e.g., `dd if=input.bin of=output.txt bs=1M`)\\n* **Are you trying to write data to a device?** (This is very dangerous!  Make absolutely sure you have the correct device specified.)\\n\\nProviding more details about your intended use will allow me to provide more specific and helpful instructions.', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'user', 'parts': ['You are a helpful AI assistant.']}, {'role': 'user', 'parts': ['wfwf\\\\']}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
